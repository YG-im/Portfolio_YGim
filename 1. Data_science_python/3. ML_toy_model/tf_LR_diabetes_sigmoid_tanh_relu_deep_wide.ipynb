{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 당뇨병 여부 진단 머신러닝\n",
    "- 총 8가지의 당뇨에 영향을 미치는 수치를 변수로 설정.\n",
    "- 이에 따른 당뇨병 여부가 라벨링된 데이터로 훈련.\n",
    "- 활성함수(sigmoid, tanh, Lelu)에 따른 정확도 차이를 연구함.\n",
    "- tanh 함수의 정확도가 가장 높았음.\n",
    "- 단순한 분류 문제이기때문에 layer의 깊이가 정확도에 악영향을 미침."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**import the pakages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "tf.set_random_seed(777)  # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cf) tanh 함수를 sigmoid함수와 유사한 모양을 가지도록 평행이동함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoid 함수 보다 조금더 step function에 가까운 tanh으로 만든 sigmoid함수 적용하면 어떨까?.\n",
    "def tanh_sigmoid(x):\n",
    "    return (1/2)*(tf.tanh(x)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 데이터 전처리."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/yongwan89/PycharmProjects/dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(759, 8) (759, 1)\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "# In the case of deep neural network, dropout (keep_prob) rate  0.7 on training, but should be 1 for testing \n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "####################################\n",
    "\n",
    "xy = np.loadtxt('data_set/diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]     #훈련 데이터 범위 지정.\n",
    "y_data = xy[:, [-1]]     #결과 라벨링 데이터 범위 지정.\n",
    "\n",
    "print(x_data.shape, y_data.shape)\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 8])   #데이터 크기에 따라 바꾸기.\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cf) tesor board활용을 위하여 name_scope와 histogram 활용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########layer##########################################################\n",
    "\n",
    "with tf.name_scope(\"Layer1\"):  #name_scope를 활용하면 게층별로 깔끔한 정리가 가능. later1이라는 이름으로 아래 층을 정리.\n",
    "    W1 = tf.Variable(tf.random_normal([8, 10]), name='weight')   #데이터 크기에 따라 바꾸기.\n",
    "    #W1=tf.get_variable(\"W1\", shape=[8, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.Variable(tf.random_normal([10]), name='bias')\n",
    "    #layer1 = tanh_sigmoid(tf.matmul(X, W1) + b1)\n",
    "    hypothesis = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "    #layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    hypothesis = tf.nn.dropout(layer1, keep_prob=keep_prob) ####################################\n",
    "# 깊게 레이어가 쌓여지면 오버피팅이 발생함. 그래서 dropout은 오버피팅을 막기위하여 랜덤하게 부분부분만 사용해서 훈련시키는거.\n",
    "\n",
    "    tf.summary.histogram(\"W1\", W1)  #어떤 텐서를 로깅할 것인지 정하기.\n",
    "    tf.summary.histogram(\"b1\", b1)\n",
    "    tf.summary.histogram(\"Layer1\", hypothesis)\n",
    "\n",
    "\n",
    "# with tf.name_scope(\"Layer2\"):  #name_scope를 활용하면 게층별로 깔끔한 정리가 가능. later1이라는 이름으로 아래 층을 정리.\n",
    "#     W2 = tf.Variable(tf.random_normal([10, 10]), name='weight')   #데이터 크기에 따라 바꾸기.\n",
    "#     #W2 = tf.get_variable(\"W2\", shape=[10, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "#     b2 = tf.Variable(tf.random_normal([10]), name='bias')\n",
    "#     #layer2 = tanh_sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "#     layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "#     #layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "#     layer2 = tf.nn.dropout(layer2, keep_prob=keep_prob)  ####################################\n",
    "\n",
    "#     tf.summary.histogram(\"W2\", W2)  #어떤 텐서를 로깅할 것인지 정하기.\n",
    "#     tf.summary.histogram(\"b2\", b2)\n",
    "#     tf.summary.histogram(\"Layer2\", layer2)\n",
    "\n",
    "# with tf.name_scope(\"Layer3\"):  #name_scope를 활용하면 게층별로 깔끔한 정리가 가능. later1이라는 이름으로 아래 층을 정리.\n",
    "#     W3 = tf.Variable(tf.random_normal([10, 1]), name='weight')   #데이터 크기에 따라 바꾸기.\n",
    "#     #W3 = tf.get_variable(\"W3\", shape=[10, 1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "#     b3 = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "#     #hypothesis = tanh_sigmoid(tf.matmul(layer4, W5) + b5)\n",
    "#     hypothesis = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "#     tf.summary.histogram(\"W3\", W3)  # 어떤 텐서를 로깅할 것인지 정하기.\n",
    "#     tf.summary.histogram(\"b3\", b3)\n",
    "#     tf.summary.histogram(\"Hypothesis\", hypothesis)\n",
    "\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cost 함수 정의 및 optimizer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function\n",
    "with tf.name_scope(\"Cost\"):\n",
    "    cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "    tf.summary.scalar(\"Cost\", cost)   #summary하려는 값이 스칼라면 tf.summary.scalar를 사용\n",
    "\n",
    "# Optimizer\n",
    "with tf.name_scope(\"Train\"):\n",
    "    #train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "    train = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# 여러가지 optimizer들이 있다. 이 중에 현재 Adam이 제일 성능이 좋은 것으로 알려져있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예측값이 0.5보다 크면 1을 출력하고 0.5보다 작으면 0을 출력해서 당뇨여부 판단."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32) # True if hypothesis>0.5 else False\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32)) # Accuracy computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 훈련 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.1369991\n",
      "200 0.53497785\n",
      "400 0.49028307\n",
      "600 0.4781747\n",
      "800 0.47420776\n",
      "1000 0.4727456\n",
      "1200 0.47215548\n",
      "1400 0.47190154\n",
      "1600 0.47178692\n",
      "1800 0.471732\n",
      "2000 0.47170392\n",
      "2200 0.47168848\n",
      "2400 0.47168\n",
      "2600 0.47167516\n",
      "2800 0.4716727\n",
      "3000 0.4716716\n",
      "3200 0.47167102\n",
      "3400 0.47167075\n",
      "3600 0.47167063\n",
      "3800 0.47167063\n",
      "4000 0.4716705\n",
      "4200 0.4716705\n",
      "4400 0.4716705\n",
      "4600 0.47167048\n",
      "4800 0.47167045\n",
      "5000 0.47167045\n",
      "5200 0.47167042\n",
      "5400 0.47167045\n",
      "5600 0.47167042\n",
      "5800 0.4716704\n",
      "6000 0.4716704\n",
      "6200 0.4716704\n",
      "6400 0.4716704\n",
      "6600 0.4716704\n",
      "6800 0.47167042\n",
      "7000 0.4716704\n",
      "7200 0.4716704\n",
      "7400 0.4716704\n",
      "7600 0.4716704\n",
      "7800 0.4716704\n",
      "8000 0.4716704\n",
      "8200 0.47167045\n",
      "8400 0.47167042\n",
      "8600 0.47167048\n",
      "8800 0.47167042\n",
      "9000 0.47167042\n",
      "9200 0.47167045\n",
      "9400 0.4716705\n",
      "9600 0.4716705\n",
      "9800 0.4716704\n",
      "10000 0.47167042\n",
      "\n",
      "Hypothesis:  [[0.3539274  0.3539279  0.35392737 ... 0.35392737 0.3539275  0.3539274 ]\n",
      " [0.95650464 0.95650476 0.95650464 ... 0.9565045  0.95650464 0.95650464]\n",
      " [0.19471748 0.1947179  0.19471745 ... 0.19471744 0.19471753 0.19471748]\n",
      " ...\n",
      " [0.86652184 0.8665224  0.86652184 ... 0.86652184 0.8665219  0.86652184]\n",
      " [0.7325784  0.732579   0.7325784  ... 0.73257834 0.73257846 0.7325784 ]\n",
      " [0.92897666 0.92897695 0.92897666 ... 0.92897666 0.9289768  0.92897666]] \n",
      "Correct (Y):  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]] \n",
      "Accuracy:  0.76943344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n5층 neural + sigmoid\\nAccuracy:  0.7220026\\n기존에서 -4% 하락.   아마 오버피팅이 문제인듯.\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # tensorboard --logdir=./logs/diabetes_tanh_deep_wide_logs_01\n",
    "    merged_summary = tf.summary.merge_all()  # 모든 Summary들을 합쳐준다.\n",
    "    writer = tf.summary.FileWriter(\"./logs/diabetes_tanh_deep_wide_logs_01\")  # summary를 어디 위치에 파일로 만들어 놓을지 정하기.\n",
    "    writer.add_graph(sess.graph)  # Show the graph  #그 파일에 그래프 좀 넣어줘.\n",
    "\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data, keep_prob: 1})\n",
    "        if step % 200 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data, keep_prob: 1})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n5층 neural + sigmoid\\nAccuracy:  0.7220026\\n기존에서 -4% 하락.   아마 오버피팅이 문제인듯.\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "0 0.82794\n",
    "200 0.755181\n",
    "400 0.726355\n",
    "600 0.705179\n",
    "800 0.686631\n",
    "...\n",
    "9600 0.492056\n",
    "9800 0.491396\n",
    "10000 0.490767\n",
    "\n",
    "...\n",
    "\n",
    " [ 1.]\n",
    " [ 1.]\n",
    " [ 1.]]\n",
    "Accuracy:  0.762846\n",
    "'''\n",
    "\n",
    "'''\n",
    "tanh_sigmoid 함수 도입하니\n",
    "Accuracy:  0.77338606\n",
    "로 정확도 1%상승.\n",
    "Hypothesis의 중요성.\n",
    "'''\n",
    "\n",
    "'''\n",
    "2층 neural + tanh_sigmoid\n",
    "Accuracy:  0.76679844\n",
    "기존에서 0.1%상승.\n",
    "'''\n",
    "\n",
    "'''\n",
    "5층 neural + tanh_sigmoid\n",
    "Accuracy:  0.7720685\n",
    "기존에서 1%상승.\n",
    "'''\n",
    "\n",
    "'''\n",
    "5층 neural + sigmoid\n",
    "Accuracy:  0.7220026\n",
    "기존에서 -4% 하락.   아마 오버피팅이 문제인듯.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
